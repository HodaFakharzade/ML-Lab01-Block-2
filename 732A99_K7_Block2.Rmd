---
title: "932A99 - Group K7 - Block 2"
author: "Hoda Fakharzadehjahromy, Otto Moen & Ravinder Reddy Atla"
date: '2020-12-01'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
```

Statement of contribution: Assignment 1 was contributed mostly by Otto, 

# 1. Assignment 1 - Ensemble methods

This assignment concerns training and analyzing random forests. 

### 1.1 - Random forest for y = x1<x2

In this section we create 1000 training datasets, each containing 100 observations with three variables. X1 and X2 are generated as random numbers from the uniform distribution with boundaries 0 and 1. Y is then created as a categorical variable based on X1 and X2 with two values; one when X1<X2 and one when X1=>X2. In total three random forests are learned on each training set, one with 1 tree, one with 10 trees and one with 100 trees. The training datasets are created with the following code:
```{r}
datasets <- list(list(x=0,
                      y=0))
set.seed(12345)
for (set in 1:1000) {
  x1<-runif(100)
  x2<-runif(100)
  trdata<-cbind(x1,x2)
  y<-as.numeric(x1<x2)
  trlabels<-as.factor(y)
  datasets[[set]] <- list(x=trdata,
                      y=trlabels)
}
```

To analyze how these random forests perform a test dataset with 1000 observations is created with the following code:
```{r}
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabels<-as.factor(y)
plot(x1,x2,col=(y+1))
```

With the datasets created the random forests can now be learned with the following code:
```{r}
trees <- c(1,10,100)
forests <- list(list(list()))
set.seed(12345)
for (set in 1:1000) {
  forests[[set]] <- list()
  for (ntrees in 1:3) {
    forests[[set]][[ntrees]] <- randomForest(x = datasets[[set]]$x, 
                                             y = datasets[[set]]$y,
                                             xtest = tedata,
                                             ytest = telabels, 
                                             ntree = trees[ntrees],
                                             nodesize = 25,
                                             keep.forest = TRUE)
  }
}
```

For these forests the misclassification errors are all calculated. We then compute the mean and variance of this error for all of the random forests with the same amount of trees:
```{r}
# Misclassification  with 1 tree
errors_onetree <- 0
for (set in 1:1000) {
  errors_onetree[set] <- forests[[set]][[1]][["test"]][["err.rate"]][[1]] 
}

# Misclassification with 10 trees
errors_tentrees <- 0
for (set in 1:1000) {
  errors_tentrees[set] <- forests[[set]][[2]][["test"]][["err.rate"]][[10]] 
}

# Misclassification with 100 trees
errors_hundredtrees <- 0
for (set in 1:1000) {
  errors_hundredtrees[set] <- forests[[set]][[3]][["test"]][["err.rate"]][[100]] 
}

c(mean(errors_onetree), mean(errors_tentrees), mean(errors_hundredtrees))
c(var(errors_onetree), var(errors_tentrees), var(errors_hundredtrees))
```
As can be observed the mean misclassification error goes down as the number of trees goes up, from 0.2 at one tree down to 0.11 at one hundred trees. Similarly the variance of the error also goes down as the number of trees goes up.

### 1.2 - Random forest for y = x1<0.5

In this section we repeat the steps taken in 1.1, but instead compute Y based on X1<0.5 instead. The new data is set up with the following code:
```{r}
# Generate 1000 data sets
datasets <- list(list(x=0,
                      y=0))
set.seed(12345)
for (set in 1:1000) {
  x1<-runif(100)
  x2<-runif(100)
  trdata<-cbind(x1,x2)
  y<-as.numeric(x1<0.5)
  trlabels<-as.factor(y)
  
  datasets[[set]] <- list(x=trdata,
                      y=trlabels)
}

# Test data
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1<0.5)
telabels<-as.factor(y)
plot(x1,x2,col=(y+1))
```
The random forests are then learned with the following code:
```{r}
trees <- c(1,10,100)
forests_two <- list(list(list()))
set.seed(12345)
for (set in 1:1000) {
  forests_two[[set]] <- list()
  for (ntrees in 1:3) {
    forests_two[[set]][[ntrees]] <- randomForest(x = datasets[[set]]$x, 
                                             y = datasets[[set]]$y,
                                             xtest = tedata,
                                             ytest = telabels, 
                                             ntree = trees[ntrees],
                                             nodesize = 25,
                                             keep.forest = TRUE)
  }
}
```

Finally the misclassification rates are computed with the following code:
```{r}
# Misclassification  with 1 tree
errors_onetree_two <- 0
for (set in 1:1000) {
  errors_onetree_two[set] <- forests_two[[set]][[1]][["test"]][["err.rate"]][[1]] 
}

# Misclassification with 10 trees
errors_tentrees_two <- 0
for (set in 1:1000) {
  errors_tentrees_two[set] <- forests_two[[set]][[2]][["test"]][["err.rate"]][[10]] 
}

# Misclassification with 100 trees
errors_hundredtrees_two <- 0
for (set in 1:1000) {
  errors_hundredtrees_two[set] <- forests_two[[set]][[3]][["test"]][["err.rate"]][[100]] 
}

c(mean(errors_onetree_two), mean(errors_tentrees_two), mean(errors_hundredtrees_two))
c(var(errors_onetree_two), var(errors_tentrees_two), var(errors_hundredtrees_two))
```
As in the previous section both the mean and variance of the error rate goes down as the number of trees goes up.

### 1.3 - Random forest for y = x1<05 & x2<0.5 | x1>0.5 & x2>0.5

Once again we repeat the exercise, this time with Y computed based on x1<05 & x2<0.5 or x1>0.5 & x2>0.5. The data is set up with the following code:
```{r}
# Generate 1000 data sets
datasets <- list(list(x=0,
                      y=0))
set.seed(12345)
for (set in 1:1000) {
  x1<-runif(100)
  x2<-runif(100)
  trdata<-cbind(x1,x2)
  y<-as.numeric((x1<0.5 & x2<0.5)|(x1>0.5 & x2>0.5))
  trlabels<-as.factor(y)
  
  datasets[[set]] <- list(x=trdata,
                      y=trlabels)
}

set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric((x1<0.5 & x2<0.5)|(x1>0.5 & x2>0.5))
telabels<-as.factor(y)
plot(x1,x2,col=(y+1))
```
The random forests are then learned with the following code:
```{r}
trees <- c(1,10,100)
forests_three <- list(list(list()))
set.seed(12345)
for (set in 1:1000) {
  forests_three[[set]] <- list()
  for (ntrees in 1:3) {
    forests_three[[set]][[ntrees]] <- randomForest(x = datasets[[set]]$x, 
                                                 y = datasets[[set]]$y,
                                                 xtest = tedata,
                                                 ytest = telabels, 
                                                 ntree = trees[ntrees],
                                                 nodesize = 12,
                                                 keep.forest = TRUE)
  }
}
```

Finally the misclassification rates are computed with the following code:
```{r}
# Misclassification  with 1 tree
errors_onetree_three <- 0
for (set in 1:1000) {
  errors_onetree_three[set] <- forests_three[[set]][[1]][["test"]][["err.rate"]][[1]] 
}

# Misclassification with 10 trees
errors_tentrees_three <- 0
for (set in 1:1000) {
  errors_tentrees_three[set] <- forests_three[[set]][[2]][["test"]][["err.rate"]][[10]] 
}

# Misclassification with 100 trees
errors_hundredtrees_three <- 0
for (set in 1:1000) {
  errors_hundredtrees_three[set] <- forests_three[[set]][[3]][["test"]][["err.rate"]][[100]] 
}

c(mean(errors_onetree_three), mean(errors_tentrees_three), mean(errors_hundredtrees_three))
c(var(errors_onetree_three), var(errors_tentrees_three), var(errors_hundredtrees_three))
```
Just as with the previous two datasets the mean and the variance of the error of the misclassification error goes down as the number of trees goes up.

### 1.4 - Questions

In this section we answer some questions on the topic of random forests.

**a) What happens with the mean and variance of the error rate when the number of trees in the random forest grows?**

Overall as the number of trees grows the mean and variance of the error rate goes down. 
  
  
**b) The third dataset represents a slightly more complicated classification problem than the first one. Still, you should get better performance for it when using sufficient trees in the random forest. Explain why you get better performance.**  

The reason why the performance improves when using more trees is because, even though the problem is a more complicated one, if the trees in general predict better than randomly guessing (error rate of 0.5) then as the number of trees increases they will on average achieve a better result. This is because the final decision for an observation is done by majority voting among the trees in the forest and so if the trees on average do better than random then a larger amount of trees will increase the chance that the majority of them voted for the right class. 
  
  
**c) Why is it desirable to have low error variance?**  

Low error variance means that a larger amount of the trees in the forest predict correct and incorrect values for the test set at a similar rate. This is desirable as it suggests that the individual trees are not overfitted on their respective training set, which if it were the case would lead to a larger variance as each tree is more dependent on the data it used for training and as such more often return different predictions for the same observation in the test set.
# 2. Assignment 2 - 
# 3. Assignment 3 - 


\newpage
# Appendix: Assignment Code

```{r, ref.label=knitr::all_labels(), echo=T, eval=F}
``` 
